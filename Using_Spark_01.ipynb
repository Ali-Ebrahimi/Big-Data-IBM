{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200501165831-0000\nKERNEL_ID = 1adcbdb4-25da-47ab-87c6-d76d0df06e96\n"
                }
            ],
            "source": "rdd = sc.parallelize(range(100))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.map(lambda x : x+1).take(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "5050"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sc.parallelize(range(1,101)).reduce(lambda a,b: a+b)"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "def gt50(i):\n    if i > 50:\n        return True\n    else:\n        return False"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "False\nTrue\n"
                }
            ],
            "source": "print(gt50(4))\nprint(gt50(51))"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "def gt50(i):\n    return i > 50"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "gt50 = lambda i: i > 50"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "False\nTrue\n"
                }
            ],
            "source": "print(gt50(4))\nprint(gt50(51))"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "#let's shuffle our list to make it a bit more interesting\nfrom random import shuffle\nl = list(range(100))\nshuffle(l)\nrdd = sc.parallelize(l)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[66,\n 84,\n 71,\n 80,\n 62,\n 56,\n 98,\n 93,\n 63,\n 52,\n 78,\n 61,\n 67,\n 79,\n 57,\n 59,\n 77,\n 73,\n 91,\n 96,\n 54,\n 55,\n 95,\n 85,\n 60,\n 89,\n 87,\n 69,\n 99,\n 64,\n 83,\n 65,\n 97,\n 76,\n 92,\n 75,\n 68,\n 70,\n 58,\n 74,\n 88,\n 53,\n 94,\n 72,\n 81,\n 51,\n 86,\n 82,\n 90]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.filter(gt50).collect()"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[66,\n 84,\n 71,\n 80,\n 62,\n 56,\n 98,\n 93,\n 63,\n 52,\n 78,\n 61,\n 67,\n 79,\n 57,\n 59,\n 77,\n 73,\n 91,\n 96,\n 54,\n 55,\n 95,\n 85,\n 60,\n 89,\n 87,\n 69,\n 99,\n 64,\n 83,\n 65,\n 97,\n 76,\n 92,\n 75,\n 68,\n 70,\n 58,\n 74,\n 88,\n 53,\n 94,\n 72,\n 81,\n 51,\n 86,\n 82,\n 90]"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.filter(lambda i: i > 50).collect()"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[84,\n 71,\n 80,\n 98,\n 93,\n 78,\n 79,\n 77,\n 73,\n 91,\n 96,\n 95,\n 85,\n 89,\n 87,\n 99,\n 83,\n 97,\n 76,\n 92,\n 75,\n 74,\n 88,\n 94,\n 72,\n 81,\n 86,\n 82,\n 90]"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.filter(lambda x: x>50 ).filter(lambda x: x>70 ).take(100)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "rdd = sc.parallelize(range(100))\nrdd2 = range(100)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "100"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sc.parallelize(range(100)).count()"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "9999999999"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "len(range(9999999999))"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": "### Spark\nfrom pyspark.sql import SQLContext\n"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": "sparkSession = SQLContext.getOrCreate(sc).sparkSession"
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o242.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.cloudant.spark. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: java.lang.ClassNotFoundException: com.cloudant.spark.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:591)\n\tat java.lang.ClassLoader.loadClassHelper(ClassLoader.java:944)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:889)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:872)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-43-a35b67721f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcloudandata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.cloudant.spark\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.host\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.username\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"washingflat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.cloudant.spark. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: java.lang.ClassNotFoundException: com.cloudant.spark.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:591)\n\tat java.lang.ClassLoader.loadClassHelper(ClassLoader.java:944)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:889)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:872)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n"
                    ]
                }
            ],
            "source": "cloudandata = sparkSession.read.format(\"com.cloudant.spark\"). \\\noption(\"cloudant.host\", \"\"). \\\noption(\"cloudant.username\", \"\"). \\\noption(\"cloudant.password\", \"\"). \\\nload(\"washingflat\")"
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+------+\n| id| value|\n+---+------+\n|  1|value1|\n|  2|value2|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n"
                }
            ],
            "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n\n# let's have a look what's inside\ndf.show()\n\n# let's print the schema\ndf.printSchema()"
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------+\n| value|\n+------+\n|value2|\n+------+\n\n"
                },
                {
                    "data": {
                        "text/plain": "'value2'"
                    },
                    "execution_count": 46,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# register dataframe as query table\ndf.createOrReplaceTempView('df_view')\n\n# execute SQL query\ndf_result = spark.sql('select value from df_view where id=2')\n\n# examine contents of result\ndf_result.show()\n\n# get result as string\ndf_result.first().value"
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Row(id=1, value='value1')"
                    },
                    "execution_count": 48,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Mean"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "rdd = sc.parallelize([101]+list(range(100)))"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "50.00990099009901"
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sum = rdd.sum()\nn = rdd.count()\nmean = sum/n\nmean"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Median"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'list' object has no attribute 'lookup'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-31-fc2f332dc41e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSortedAndIndexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mindex1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lookup'"
                    ]
                }
            ],
            "source": "SortedAndIndexed = rdd.sortBy(lambda x : x).zipWithIndex().map(lambda val: (val[1], val[0])).collect()\nn = len(SortedAndIndexed)\nif (n % 2 == 1):\n    index = (n-1)/2\n    print (SortedAndIndexed.lookup(index))\nelse:\n    index1 = (n/2)-1\n    index2 = n/2\n    value1 = SortedAndIndexed.lookup(index1)\n    value2 = SortedAndIndexed.lookup(index2)\n    print ((value1+value1)/2)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "51.0 is not in list",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-21-dd72f57c66b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSortedAndIndexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m: 51.0 is not in list"
                    ]
                }
            ],
            "source": "SortedAndIndexed.index(index1)"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 0),\n (1, 1),\n (2, 2),\n (3, 3),\n (4, 4),\n (5, 5),\n (6, 6),\n (7, 7),\n (8, 8),\n (9, 9),\n (10, 10),\n (11, 11),\n (12, 12),\n (13, 13),\n (14, 14),\n (15, 15),\n (16, 16),\n (17, 17),\n (18, 18),\n (19, 19),\n (20, 20),\n (21, 21),\n (22, 22),\n (23, 23),\n (24, 24),\n (25, 25),\n (26, 26),\n (27, 27),\n (28, 28),\n (29, 29),\n (30, 30),\n (31, 31),\n (32, 32),\n (33, 33),\n (34, 34),\n (35, 35),\n (36, 36),\n (37, 37),\n (38, 38),\n (39, 39),\n (40, 40),\n (41, 41),\n (42, 42),\n (43, 43),\n (44, 44),\n (45, 45),\n (46, 46),\n (47, 47),\n (48, 48),\n (49, 49),\n (50, 50),\n (51, 51),\n (52, 52),\n (53, 53),\n (54, 54),\n (55, 55),\n (56, 56),\n (57, 57),\n (58, 58),\n (59, 59),\n (60, 60),\n (61, 61),\n (62, 62),\n (63, 63),\n (64, 64),\n (65, 65),\n (66, 66),\n (67, 67),\n (68, 68),\n (69, 69),\n (70, 70),\n (71, 71),\n (72, 72),\n (73, 73),\n (74, 74),\n (75, 75),\n (76, 76),\n (77, 77),\n (78, 78),\n (79, 79),\n (80, 80),\n (81, 81),\n (82, 82),\n (83, 83),\n (84, 84),\n (85, 85),\n (86, 86),\n (87, 87),\n (88, 88),\n (89, 89),\n (90, 90),\n (91, 91),\n (92, 92),\n (93, 93),\n (94, 94),\n (95, 95),\n (96, 96),\n (97, 97),\n (98, 98),\n (99, 99),\n (100, 101)]"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "SortedAndIndexed"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}