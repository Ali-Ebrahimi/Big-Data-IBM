{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Regression & Classification Models using Spark\n# Ali Ebrahimi\n## Ali.Ebrahimi68@gmail.com"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Reaging Data & ETL"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\r\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark==2.4.5) (0.10.7)\r\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-06 17:06:26--  http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\nResolving max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to max-training-data.s3-api.us-geo.objectstorage.softlayer.net (max-training-data.s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2575759 (2.5M) [application/x-tar]\nSaving to: \u2018jfk_weather.tar.gz\u2019\n\n100%[======================================>] 2,575,759   --.-K/s   in 0.04s   \n\n2020-05-06 17:06:26 (58.2 MB/s) - \u2018jfk_weather.tar.gz\u2019 saved [2575759/2575759]\n\n./._jfk_weather.csv\njfk_weather.csv\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f jfk_weather*\n\n# download the file containing the data in CSV format\n!wget http://max-training-data.s3-api.us-geo.objectstorage.softlayer.net/noaa-weather/jfk_weather.tar.gz\n\n# extract the data\n!tar xvfz jfk_weather.tar.gz\n    \n# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('jfk_weather.csv')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "import random\nrandom.seed(42)\n\nfrom pyspark.sql.functions import translate, col\n\ndf_cleaned = df \\\n    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n\ndf_cleaned =   df_cleaned \\\n                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n\ndf_filtered = df_cleaned.filter(\"\"\"\n    HOURLYWindSpeed <> 0\n    and HOURLYWindSpeed IS NOT NULL\n    and HOURLYWindDirection IS NOT NULL\n    and HOURLYStationPressure IS NOT NULL\n    and HOURLYPressureTendency IS NOT NULL\n    and HOURLYPrecip IS NOT NULL\n    and HOURLYRelativeHumidity IS NOT NULL\n    and HOURLYDRYBULBTEMPC IS NOT NULL\n\"\"\")"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 1.        , -0.01324305],\n       [-0.01324305,  1.        ]])"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYPressureTendency\"],\n                                  outputCol=\"features\")\ndf_pipeline = vectorAssembler.transform(df_filtered)\nfrom pyspark.ml.stat import Correlation\nCorrelation.corr(df_pipeline,\"features\").head()[0].toArray()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "splits = df_filtered.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml import Pipeline\nimport time\n\nvectorAssembler = VectorAssembler(inputCols=[\n                                    \"HOURLYWindDirection\",\n                                    \"ELEVATION\",\n                                    \"HOURLYStationPressure\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Regression Models"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "def regression_metrics(prediction):\n    from pyspark.ml.evaluation import RegressionEvaluator\n    evaluator = RegressionEvaluator(\n    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = evaluator.evaluate(prediction)\n    print(\"RMSE on test data = %g\" % rmse)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## LinearRegression"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.35498\n47.53692555427551\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import LinearRegression\n\n\nlr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features', maxIter=100, regParam=0.0, elasticNetParam=0.0)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.58542\n35.339702129364014\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import LinearRegression\n\n\nlr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features_norm', maxIter=100, regParam=0.0, elasticNetParam=0.0)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## GeneralizedLinearRegression"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.35498\n19.722795963287354\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import GeneralizedLinearRegression\n\n\nglr = GeneralizedLinearRegression(labelCol=\"HOURLYWindSpeed\", family=\"gaussian\", link=\"identity\", linkPredictionCol=\"p\")\npipeline = Pipeline(stages=[vectorAssembler, normalizer,glr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## DecisionTreeRegressor"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.29311\n22.255017280578613\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import DecisionTreeRegressor\n\n\ndt = DecisionTreeRegressor(labelCol=\"HOURLYWindSpeed\", maxDepth=2, varianceCol=\"variance\")\npipeline = Pipeline(stages=[vectorAssembler, normalizer,dt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## RandomForestRegressor"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.33701\n23.598531007766724\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\n\nrf = RandomForestRegressor(labelCol=\"HOURLYWindSpeed\", numTrees=30)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,rf])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## GBTRegressor"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "RMSE on test data = 5.13771\n132.11187434196472\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.regression import GBTRegressor\ngbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\npipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nregression_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Classification Models"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import Bucketizer, OneHotEncoder\nbucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\nencoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "def classification_metrics(prediction):\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n    accuracy = mcEval.evaluate(prediction)\n    print(\"Accuracy on test data = %g\" % accuracy)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## LogisticRegression"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.688165\n18.724087715148926\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## RandomForest"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.721883\n29.60146141052246\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=30)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## GBTClassifier"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.730708\n108.23918104171753\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## DecisionTreeClassifier"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.704684\n18.243031978607178\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxDepth=2)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,dt])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## MultilayerPerceptronClassifier"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "14.210877895355225\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nmlp = MultilayerPerceptronClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100, layers=[7, 7, 2], blockSize=1)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,mlp])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\n# classification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# SMV"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.69699\n519.3648540973663\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import LinearSVC\nsvm = LinearSVC(labelCol=\"HOURLYWindDirectionBucketized\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,svm])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## NaiveBayes"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.658294\n20.646251678466797\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes(labelCol=\"HOURLYWindDirectionBucketized\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\",\"ELEVATION\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,nb])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## OneVsRest"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accuracy on test data = 0.683865\n46.43319320678711\n"
                }
            ],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import OneVsRest\n\nlr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\novr = OneVsRest(labelCol=\"HOURLYWindDirectionBucketized\", classifier=lr)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,ovr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## FMClassifier"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "t = time.time()\n\nfrom pyspark.ml.classification import FMClassifier\n\nsvm = FMClassifier(labelCol=\"HOURLYWindDirectionBucketized\", factorSize=2)\n\nvectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n                                  outputCol=\"features\")\n\npipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,svm])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nclassification_metrics(prediction)\n\nelapsed = time.time() - t\nprint(elapsed)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}